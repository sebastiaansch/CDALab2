{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.api import qqplot\n",
    "from pmdarima.arima import auto_arima\n",
    "import multiprocessing as mp\n",
    "import saxpy\n",
    "from saxpy.sax import sax_via_window\n",
    "from saxpy.alphabet import cuts_for_asize\n",
    "from saxpy.znorm import znorm\n",
    "from saxpy.paa import paa\n",
    "from saxpy.sax import ts_to_string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"data/BATADAL_dataset03.csv\")\n",
    "df2 = pd.read_csv(\"data/BATADAL_dataset04.csv\")\n",
    "df2.columns = df2.columns.str.replace(\" \", \"\")\n",
    "dftest = pd.read_csv(\"data/BATADAL_test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the right attack flags\n",
    "new_attack_df2 = df2[['DATETIME', 'ATT_FLAG']].copy().set_index(\"DATETIME\")\n",
    "new_attack_df2['ATT_FLAG'] = 0\n",
    "\n",
    "attack_points = [\n",
    "    [\"13/09/16 23\", \"16/09/16 00\"],\n",
    "    [\"26/09/16 11\", \"27/09/16 10\"],\n",
    "    [\"09/10/16 09\", \"11/10/16 20\"],\n",
    "    [\"29/10/16 19\", \"02/11/16 16\"],\n",
    "    [\"26/11/16 17\", \"29/11/16 04\"],\n",
    "    [\"06/12/16 07\", \"10/12/16 04\"],\n",
    "    [\"14/12/16 15\", \"19/12/16 04\"]\n",
    "]\n",
    "\n",
    "for attack_point in attack_points:\n",
    "    new_attack_df2.loc[attack_point[0]:attack_point[1], 'ATT_FLAG'] = 1\n",
    "    \n",
    "new_attack_df2.reset_index(inplace=True)\n",
    "\n",
    "new_attack_df_test = dftest[['DATETIME']].copy().set_index(\"DATETIME\")\n",
    "new_attack_df_test['ATT_FLAG'] = 0\n",
    "\n",
    "attack_points = [\n",
    "    [\"16/01/17 09\", \"19/01/17 06\"],\n",
    "    [\"30/01/17 08\", \"02/02/17 00\"],\n",
    "    [\"09/02/17 03\", \"10/02/17 09\"],\n",
    "    [\"12/02/17 01\", \"13/02/17 07\"],\n",
    "    [\"24/02/17 05\", \"28/02/17 08\"],\n",
    "    [\"10/03/17 14\", \"13/03/17 21\"],\n",
    "    [\"25/03/17 20\", \"27/03/17 01\"]\n",
    "]\n",
    "\n",
    "for attack_point in attack_points:\n",
    "    new_attack_df_test.loc[attack_point[0]:attack_point[1], 'ATT_FLAG'] = 1\n",
    "    \n",
    "new_attack_df_test.reset_index(inplace=True)\n",
    "\n",
    "df2['ATT_FLAG'] = new_attack_df2['ATT_FLAG']\n",
    "dftest['ATT_FLAG'] = new_attack_df_test['ATT_FLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.drop(\"DATETIME\",1)\n",
    "\n",
    "dfy = df2[\"ATT_FLAG\"]\n",
    "dfy = dfy.replace(-999,0)\n",
    "x = []\n",
    "x = pd.DataFrame(x)\n",
    "for index in range(0,len(data.columns)):\n",
    "    indexname = \"{}\".format(data.columns[index])\n",
    "    dat = data[indexname].values\n",
    "    amount_of_levels = 3\n",
    "    window_size = 2\n",
    "\n",
    "    discrete_signal = []\n",
    "    for t in range(len(dat)-window_size):\n",
    "        dat_znorm = znorm(dat[t:t+window_size])\n",
    "        discrete_signal.append(ts_to_string(dat_znorm, cuts_for_asize(amount_of_levels)))\n",
    "\n",
    "    x[indexname] = discrete_signal\n",
    "saxxeddata = x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saxxeddata = saxxeddata.drop(\"ATT_FLAG\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saxxeddata[\"output\"] = dfy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_for_onehot = saxxeddata.columns\n",
    "new_df = pd.DataFrame([])\n",
    "\n",
    "for target in targets_for_onehot:\n",
    "    temp = pd.get_dummies(saxxeddata[target])\n",
    "    new_df = pd.concat([new_df, temp],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfy = df2[\"ATT_FLAG\"]\n",
    "# dfy = dfy.replace(-999,0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "data= df2\n",
    "datay = dfy\n",
    "data = data.drop(\"DATETIME\",axis=1)\n",
    "data = data.drop(\"ATT_FLAG\",axis=1)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data, dfy, test_size = 0.2)\n",
    "xTrain = torch.tensor(xTrain.values,dtype=torch.float)\n",
    "yTrain = torch.tensor(yTrain.values,dtype=torch.float)\n",
    "xTest = torch.tensor(xTest.values,dtype=torch.float)\n",
    "yTest = torch.tensor(yTest.values,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0  train loss:  0.9440285563468933\n",
      "test loss:  0.9617223143577576\n",
      "Round:  1  train loss:  0.9440260529518127\n",
      "test loss:  0.9617197513580322\n",
      "Round:  2  train loss:  0.9439995884895325\n",
      "test loss:  0.9616899490356445\n",
      "Round:  3  train loss:  0.9437834620475769\n",
      "test loss:  0.9614313840866089\n",
      "Round:  4  train loss:  0.9423401951789856\n",
      "test loss:  0.9597901701927185\n",
      "Round:  5  train loss:  0.9352802634239197\n",
      "test loss:  0.9528482556343079\n",
      "Round:  6  train loss:  0.9109997153282166\n",
      "test loss:  0.9290820360183716\n",
      "Round:  7  train loss:  0.8478570580482483\n",
      "test loss:  0.8570886254310608\n",
      "Round:  8  train loss:  0.687922477722168\n",
      "test loss:  0.6812738180160522\n",
      "Round:  9  train loss:  0.41766107082366943\n",
      "test loss:  0.4017239212989807\n",
      "Round:  10  train loss:  0.16146795451641083\n",
      "test loss:  0.14689281582832336\n",
      "Round:  11  train loss:  0.07924968004226685\n",
      "test loss:  0.06164178624749184\n",
      "Round:  12  train loss:  0.05662158876657486\n",
      "test loss:  0.03906097263097763\n",
      "Round:  13  train loss:  0.055762939155101776\n",
      "test loss:  0.038161665201187134\n",
      "Round:  14  train loss:  0.05591587349772453\n",
      "test loss:  0.038243260234594345\n",
      "Round:  15  train loss:  0.055956535041332245\n",
      "test loss:  0.038267891854047775\n",
      "Round:  16  train loss:  0.055966589599847794\n",
      "test loss:  0.03827431797981262\n",
      "Round:  17  train loss:  0.05596956983208656\n",
      "test loss:  0.0382763147354126\n",
      "Round:  18  train loss:  0.055970583111047745\n",
      "test loss:  0.03827701881527901\n",
      "Round:  19  train loss:  0.055970966815948486\n",
      "test loss:  0.03827729448676109\n",
      "Round:  20  train loss:  0.05597112327814102\n",
      "test loss:  0.03827740624547005\n",
      "Round:  21  train loss:  0.05597119405865669\n",
      "test loss:  0.038277458399534225\n",
      "Round:  22  train loss:  0.05597122758626938\n",
      "test loss:  0.038277484476566315\n",
      "Round:  23  train loss:  0.05597124248743057\n",
      "test loss:  0.03827749565243721\n",
      "Round:  24  train loss:  0.05597125366330147\n",
      "test loss:  0.03827750310301781\n",
      "Round:  25  train loss:  0.055971257388591766\n",
      "test loss:  0.038277506828308105\n",
      "Round:  26  train loss:  0.055971261113882065\n",
      "test loss:  0.038277506828308105\n",
      "Round:  27  train loss:  0.055971261113882065\n",
      "test loss:  0.038277510553598404\n",
      "Round:  28  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  29  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  30  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  31  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  32  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  33  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  34  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  35  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  36  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  37  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  38  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  39  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  40  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  41  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  42  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  43  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  44  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  45  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  46  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  47  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  48  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  49  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "var = df2.drop([\"DATETIME\",\"ATT_FLAG\"],axis=1).shape[1]\n",
    "n_in, n_h, n_out, batch_size = var, 10, 1, 5\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())\n",
    "criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "for epoch in range(50):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(xTrain)\n",
    "    y_pred_test = model(xTest)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, yTrain)\n",
    "    testloss = criterion(y_pred_test,yTest)\n",
    "    print('Round: ', epoch,' train loss: ', loss.item())\n",
    "    print(\"test loss: \", testloss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = y_pred[y_pred>0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "target_columns = list(df2.columns.drop([\"DATETIME\",\"ATT_FLAG\"]))\n",
    "\n",
    "train_df = df2.copy()\n",
    "sliding_dfs = []\n",
    "\n",
    "for column in target_columns:\n",
    "    target_df = train_df[[column]]\n",
    "    sliding_width = 10\n",
    "    sliding_df = pd.concat([target_df.shift(t_delta) for t_delta in range(sliding_width)], axis=1)\n",
    "    sliding_df.columns = ['L_T1_' + str(t_delta) for t_delta in range(sliding_width)]\n",
    "    sliding_dfs.append(sliding_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.concat(sliding_dfs, axis=1)\n",
    "train_labels = train_df['ATT_FLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0  train loss:  0.153767928481102\n",
      "test loss:  0.15595974028110504\n",
      "Round:  1  train loss:  0.1554887890815735\n",
      "test loss:  0.14906644821166992\n",
      "Round:  2  train loss:  0.15279874205589294\n",
      "test loss:  0.15978364646434784\n",
      "Round:  3  train loss:  0.15522794425487518\n",
      "test loss:  0.15024523437023163\n",
      "Round:  4  train loss:  0.15515971183776855\n",
      "test loss:  0.15039202570915222\n",
      "Round:  5  train loss:  0.15313602983951569\n",
      "test loss:  0.15866871178150177\n",
      "Round:  6  train loss:  0.15487299859523773\n",
      "test loss:  0.15163946151733398\n",
      "Round:  7  train loss:  0.154394268989563\n",
      "test loss:  0.1538282036781311\n",
      "Round:  8  train loss:  0.1550922691822052\n",
      "test loss:  0.150662362575531\n",
      "Round:  9  train loss:  0.15446734428405762\n",
      "test loss:  0.1531485915184021\n",
      "Round:  10  train loss:  0.15634137392044067\n",
      "test loss:  0.14566658437252045\n",
      "Round:  11  train loss:  0.15656863152980804\n",
      "test loss:  0.14481352269649506\n",
      "Round:  12  train loss:  0.15485621988773346\n",
      "test loss:  0.15159446001052856\n",
      "Round:  13  train loss:  0.15129175782203674\n",
      "test loss:  0.16587363183498383\n",
      "Round:  14  train loss:  0.15474630892276764\n",
      "test loss:  0.15228325128555298\n",
      "Round:  15  train loss:  nan\n",
      "test loss:  0.15552259981632233\n",
      "Round:  16  train loss:  0.152239590883255\n",
      "test loss:  0.1620505154132843\n",
      "Round:  17  train loss:  0.15164700150489807\n",
      "test loss:  0.16433332860469818\n",
      "Round:  18  train loss:  0.15366709232330322\n",
      "test loss:  0.1564970463514328\n",
      "Round:  19  train loss:  0.15558458864688873\n",
      "test loss:  0.1488208919763565\n",
      "Round:  20  train loss:  0.15097445249557495\n",
      "test loss:  0.16687427461147308\n",
      "Round:  21  train loss:  0.1530739814043045\n",
      "test loss:  0.15882787108421326\n",
      "Round:  22  train loss:  0.1533925086259842\n",
      "test loss:  0.15746189653873444\n",
      "Round:  23  train loss:  0.15203557908535004\n",
      "test loss:  0.16276156902313232\n",
      "Round:  24  train loss:  0.1549675315618515\n",
      "test loss:  0.15117807686328888\n",
      "Round:  25  train loss:  0.15580232441425323\n",
      "test loss:  0.14773182570934296\n",
      "Round:  26  train loss:  0.15330752730369568\n",
      "test loss:  0.15779906511306763\n",
      "Round:  27  train loss:  nan\n",
      "test loss:  0.14020654559135437\n",
      "Round:  28  train loss:  0.1567992866039276\n",
      "test loss:  0.14378421008586884\n",
      "Round:  29  train loss:  0.1517837792634964\n",
      "test loss:  0.16382154822349548\n",
      "Round:  30  train loss:  0.15291546285152435\n",
      "test loss:  0.1593656837940216\n",
      "Round:  31  train loss:  0.15126676857471466\n",
      "test loss:  0.16609683632850647\n",
      "Round:  32  train loss:  0.15447166562080383\n",
      "test loss:  0.15317076444625854\n",
      "Round:  33  train loss:  0.15621143579483032\n",
      "test loss:  0.1461235135793686\n",
      "Round:  34  train loss:  0.15480853617191315\n",
      "test loss:  0.1517801284790039\n",
      "Round:  35  train loss:  0.15392285585403442\n",
      "test loss:  0.15540875494480133\n",
      "Round:  36  train loss:  0.15251803398132324\n",
      "test loss:  0.16112422943115234\n",
      "Round:  37  train loss:  0.1534041464328766\n",
      "test loss:  0.15739032626152039\n",
      "Round:  38  train loss:  0.15621113777160645\n",
      "test loss:  0.14613886177539825\n",
      "Round:  39  train loss:  0.15274305641651154\n",
      "test loss:  0.15999023616313934\n",
      "Round:  40  train loss:  0.15541613101959229\n",
      "test loss:  0.1493774801492691\n",
      "Round:  41  train loss:  0.15568996965885162\n",
      "test loss:  0.14817722141742706\n",
      "Round:  42  train loss:  0.15523768961429596\n",
      "test loss:  0.15009251236915588\n",
      "Round:  43  train loss:  0.15669681131839752\n",
      "test loss:  0.14451390504837036\n",
      "Round:  44  train loss:  0.15376020967960358\n",
      "test loss:  0.15598493814468384\n",
      "Round:  45  train loss:  0.1555992215871811\n",
      "test loss:  0.1486114263534546\n",
      "Round:  46  train loss:  0.15260879695415497\n",
      "test loss:  0.16051001846790314\n",
      "Round:  47  train loss:  0.15163423120975494\n",
      "test loss:  0.16471950709819794\n",
      "Round:  48  train loss:  0.1505482941865921\n",
      "test loss:  0.16849279403686523\n",
      "Round:  49  train loss:  0.15390172600746155\n",
      "test loss:  0.15541678667068481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "var = train_features.shape[1]\n",
    "n_in, n_h, n_out = var, 10, 1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())\n",
    "criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "for epoch in range(100):\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(train_features, train_labels, test_size = 0.2)\n",
    "    xTrain = torch.tensor(xTrain.values,dtype=torch.float)\n",
    "    yTrain = torch.tensor(yTrain.values,dtype=torch.float)\n",
    "    xTest = torch.tensor(xTest.values,dtype=torch.float)\n",
    "    yTest = torch.tensor(yTest.values,dtype=torch.float)\n",
    "    # Forward Propagation\n",
    "    \n",
    "    y_pred = model(xTrain)\n",
    "    y_pred_test = model(xTest)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, yTrain)\n",
    "    testloss = criterion(y_pred_test,yTest)\n",
    "    print('Round: ', epoch,' train loss: ', loss.item())\n",
    "    print(\"test loss: \", testloss.item())\n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3b5468fb00>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "threshold = 0.95\n",
    "sns.lineplot(data=pd.Series([x[0] for x in (y_pred>threshold).tolist()]), ax=ax)\n",
    "sns.lineplot(data=train_labels, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
