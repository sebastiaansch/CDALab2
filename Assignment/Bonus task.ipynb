{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.api import qqplot\n",
    "from pmdarima.arima import auto_arima\n",
    "import multiprocessing as mp\n",
    "import saxpy\n",
    "from saxpy.sax import sax_via_window\n",
    "from saxpy.alphabet import cuts_for_asize\n",
    "from saxpy.znorm import znorm\n",
    "from saxpy.paa import paa\n",
    "from saxpy.sax import ts_to_string\n",
    "\n",
    "df1 = pd.read_csv(\"data/BATADAL_dataset03.csv\")\n",
    "df2 = pd.read_csv(\"data/BATADAL_dataset04.csv\")\n",
    "df2.columns = df2.columns.str.replace(\" \", \"\")\n",
    "dftest = pd.read_csv(\"data/BATADAL_test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.drop(\"DATETIME\",1)\n",
    "\n",
    "dfy = df2[\"ATT_FLAG\"]\n",
    "dfy = dfy.replace(-999,0)\n",
    "x = []\n",
    "x = pd.DataFrame(x)\n",
    "for index in range(0,len(data.columns)):\n",
    "    indexname = \"{}\".format(data.columns[index])\n",
    "    dat = data[indexname].values\n",
    "    amount_of_levels = 3\n",
    "    window_size = 2\n",
    "\n",
    "    discrete_signal = []\n",
    "    for t in range(len(dat)-window_size):\n",
    "        dat_znorm = znorm(dat[t:t+window_size])\n",
    "        discrete_signal.append(ts_to_string(dat_znorm, cuts_for_asize(amount_of_levels)))\n",
    "\n",
    "    x[indexname] = discrete_signal\n",
    "saxxeddata = x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saxxeddata = saxxeddata.drop(\"ATT_FLAG\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saxxeddata[\"output\"] = dfy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_for_onehot = saxxeddata.columns\n",
    "new_df = pd.DataFrame([])\n",
    "\n",
    "for target in targets_for_onehot:\n",
    "    temp = pd.get_dummies(saxxeddata[target])\n",
    "    new_df = pd.concat([new_df, temp],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfy = df2[\"ATT_FLAG\"]\n",
    "# dfy = dfy.replace(-999,0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "data= df2\n",
    "datay = dfy\n",
    "data = data.drop(\"DATETIME\",axis=1)\n",
    "data = data.drop(\"ATT_FLAG\",axis=1)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data, dfy, test_size = 0.2)\n",
    "xTrain = torch.tensor(xTrain.values,dtype=torch.float)\n",
    "yTrain = torch.tensor(yTrain.values,dtype=torch.float)\n",
    "xTest = torch.tensor(xTest.values,dtype=torch.float)\n",
    "yTest = torch.tensor(yTest.values,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0  train loss:  0.9440285563468933\n",
      "test loss:  0.9617223143577576\n",
      "Round:  1  train loss:  0.9440260529518127\n",
      "test loss:  0.9617197513580322\n",
      "Round:  2  train loss:  0.9439995884895325\n",
      "test loss:  0.9616899490356445\n",
      "Round:  3  train loss:  0.9437834620475769\n",
      "test loss:  0.9614313840866089\n",
      "Round:  4  train loss:  0.9423401951789856\n",
      "test loss:  0.9597901701927185\n",
      "Round:  5  train loss:  0.9352802634239197\n",
      "test loss:  0.9528482556343079\n",
      "Round:  6  train loss:  0.9109997153282166\n",
      "test loss:  0.9290820360183716\n",
      "Round:  7  train loss:  0.8478570580482483\n",
      "test loss:  0.8570886254310608\n",
      "Round:  8  train loss:  0.687922477722168\n",
      "test loss:  0.6812738180160522\n",
      "Round:  9  train loss:  0.41766107082366943\n",
      "test loss:  0.4017239212989807\n",
      "Round:  10  train loss:  0.16146795451641083\n",
      "test loss:  0.14689281582832336\n",
      "Round:  11  train loss:  0.07924968004226685\n",
      "test loss:  0.06164178624749184\n",
      "Round:  12  train loss:  0.05662158876657486\n",
      "test loss:  0.03906097263097763\n",
      "Round:  13  train loss:  0.055762939155101776\n",
      "test loss:  0.038161665201187134\n",
      "Round:  14  train loss:  0.05591587349772453\n",
      "test loss:  0.038243260234594345\n",
      "Round:  15  train loss:  0.055956535041332245\n",
      "test loss:  0.038267891854047775\n",
      "Round:  16  train loss:  0.055966589599847794\n",
      "test loss:  0.03827431797981262\n",
      "Round:  17  train loss:  0.05596956983208656\n",
      "test loss:  0.0382763147354126\n",
      "Round:  18  train loss:  0.055970583111047745\n",
      "test loss:  0.03827701881527901\n",
      "Round:  19  train loss:  0.055970966815948486\n",
      "test loss:  0.03827729448676109\n",
      "Round:  20  train loss:  0.05597112327814102\n",
      "test loss:  0.03827740624547005\n",
      "Round:  21  train loss:  0.05597119405865669\n",
      "test loss:  0.038277458399534225\n",
      "Round:  22  train loss:  0.05597122758626938\n",
      "test loss:  0.038277484476566315\n",
      "Round:  23  train loss:  0.05597124248743057\n",
      "test loss:  0.03827749565243721\n",
      "Round:  24  train loss:  0.05597125366330147\n",
      "test loss:  0.03827750310301781\n",
      "Round:  25  train loss:  0.055971257388591766\n",
      "test loss:  0.038277506828308105\n",
      "Round:  26  train loss:  0.055971261113882065\n",
      "test loss:  0.038277506828308105\n",
      "Round:  27  train loss:  0.055971261113882065\n",
      "test loss:  0.038277510553598404\n",
      "Round:  28  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  29  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  30  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  31  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  32  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  33  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  34  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  35  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  36  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  37  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  38  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  39  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  40  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  41  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  42  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  43  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  44  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  45  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  46  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  47  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  48  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n",
      "Round:  49  train loss:  0.05597126483917236\n",
      "test loss:  0.038277510553598404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "var = df2.drop([\"DATETIME\",\"ATT_FLAG\"],axis=1).shape[1]\n",
    "n_in, n_h, n_out, batch_size = var, 10, 1, 5\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())\n",
    "criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "for epoch in range(50):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(xTrain)\n",
    "    y_pred_test = model(xTest)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, yTrain)\n",
    "    testloss = criterion(y_pred_test,yTest)\n",
    "    print('Round: ', epoch,' train loss: ', loss.item())\n",
    "    print(\"test loss: \", testloss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = y_pred[y_pred>0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
